# markov_decision_process_program
Markov Decision Processes are mathematical frameworks used to model a sequential decision-making problem.  At every step in the process, the “agent” must choose an action to take at that step, and then receives a reward after arriving (usually probabilistically) in some new condition, or state, as a result of having taken its chosen action.  The goal of an MDP is to find the optimal action to take from any state in the process in order to maximize long-term rewards.  The goal of this assignment is to implement an MDP, and the value iteration algorithm to solve for the optimal policy.
When deciding which action to take in a given state, the agent needs to have an approximation of the reward it will get from taking that action in that state.  This is typically done by calculating the expected value of taking the action in the current state.  From some state s, the expected value of taking action a is E(s,a)=∑_s'▒T(s,a,s^' )R(s') .  In other words, the expected value of taking action a in state s is equal to the sum of rewards from each possible next (or successor) state, multiplied by the probability of arriving in that state.  This is only a simple one-step lookahead method that only gives the agent information about the reward it can expect to receive, but only at the next time step, no further.  To find a path all the way through the state space, the agent will need to use a more intelligent approach: value iteration.
The value iteration algorithm works by 1) initializing estimated utility values for each possible state, 2) iteratively updating those values until they converge, and 3) using the final utility values to determine the best path through the state space, and thus the best policy.
Requirements
The agent in this assignment exists in a grid world.  It can move up, down, left, or right (^, v, <, >), and will have the same behavior as the one discussed in class: whenever the agent chooses an action, it has a 70% chance of successfully completing it, and a 10% chance (each) in accidentally moving in any of the other three directions.  You must implement a class that takes as input a list of lists (our grid) filled with rewards for each state.  The class must then carry out the value iteration algorithm and determine the best policy, or the best action associated with each state.  Your program should output a new list of lists, but instead of containing rewards for each state, it should contain the optimal action for each state.  If there is more than one optimal action in a state, you may choose exactly one of them for the output.  The equation for the value iteration update is:
U_(i+1) (s)=R(s)+γ  max┬a⁡∑_s'▒〖T(s,a,s') U_i (s') 〗
